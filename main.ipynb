{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\luan\\documents\\dl\\tft\\.venv\\lib\\site-packages (from -r requirements.txt (line 1)) (2.2.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\luan\\documents\\dl\\tft\\.venv\\lib\\site-packages (from -r requirements.txt (line 2)) (2.1.2)\n",
      "Requirement already satisfied: kagglehub in c:\\users\\luan\\documents\\dl\\tft\\.venv\\lib\\site-packages (from -r requirements.txt (line 3)) (0.3.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\luan\\documents\\dl\\tft\\.venv\\lib\\site-packages (from -r requirements.txt (line 4)) (1.14.1)\n",
      "Collecting yfinance (from -r requirements.txt (line 5))\n",
      "  Using cached yfinance-0.2.50-py2.py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting scikit-learn (from -r requirements.txt (line 6))\n",
      "  Downloading scikit_learn-1.6.0-cp311-cp311-win_amd64.whl.metadata (15 kB)\n",
      "Collecting pandas_ta (from -r requirements.txt (line 7))\n",
      "  Using cached pandas_ta-0.3.14b0-py3-none-any.whl\n",
      "Requirement already satisfied: transformers in c:\\users\\luan\\documents\\dl\\tft\\.venv\\lib\\site-packages (from -r requirements.txt (line 8)) (4.46.0)\n",
      "Collecting requests_ratelimiter (from -r requirements.txt (line 9))\n",
      "  Using cached requests_ratelimiter-0.7.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\luan\\documents\\dl\\tft\\.venv\\lib\\site-packages (from pandas->-r requirements.txt (line 1)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\luan\\documents\\dl\\tft\\.venv\\lib\\site-packages (from pandas->-r requirements.txt (line 1)) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\luan\\documents\\dl\\tft\\.venv\\lib\\site-packages (from pandas->-r requirements.txt (line 1)) (2024.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\luan\\documents\\dl\\tft\\.venv\\lib\\site-packages (from kagglehub->-r requirements.txt (line 3)) (24.1)\n",
      "Requirement already satisfied: requests in c:\\users\\luan\\documents\\dl\\tft\\.venv\\lib\\site-packages (from kagglehub->-r requirements.txt (line 3)) (2.32.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\luan\\documents\\dl\\tft\\.venv\\lib\\site-packages (from kagglehub->-r requirements.txt (line 3)) (4.66.6)\n",
      "Collecting multitasking>=0.0.7 (from yfinance->-r requirements.txt (line 5))\n",
      "  Using cached multitasking-0.0.11-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting lxml>=4.9.1 (from yfinance->-r requirements.txt (line 5))\n",
      "  Using cached lxml-5.3.0-cp311-cp311-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: platformdirs>=2.0.0 in c:\\users\\luan\\documents\\dl\\tft\\.venv\\lib\\site-packages (from yfinance->-r requirements.txt (line 5)) (4.3.6)\n",
      "Collecting frozendict>=2.3.4 (from yfinance->-r requirements.txt (line 5))\n",
      "  Using cached frozendict-2.4.6-py311-none-any.whl.metadata (23 kB)\n",
      "Collecting peewee>=3.16.2 (from yfinance->-r requirements.txt (line 5))\n",
      "  Using cached peewee-3.17.8-py3-none-any.whl\n",
      "Collecting beautifulsoup4>=4.11.1 (from yfinance->-r requirements.txt (line 5))\n",
      "  Using cached beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting html5lib>=1.1 (from yfinance->-r requirements.txt (line 5))\n",
      "  Using cached html5lib-1.1-py2.py3-none-any.whl.metadata (16 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->-r requirements.txt (line 6))\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->-r requirements.txt (line 6))\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\luan\\documents\\dl\\tft\\.venv\\lib\\site-packages (from transformers->-r requirements.txt (line 8)) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\luan\\documents\\dl\\tft\\.venv\\lib\\site-packages (from transformers->-r requirements.txt (line 8)) (0.26.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\luan\\documents\\dl\\tft\\.venv\\lib\\site-packages (from transformers->-r requirements.txt (line 8)) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\luan\\documents\\dl\\tft\\.venv\\lib\\site-packages (from transformers->-r requirements.txt (line 8)) (2024.9.11)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\luan\\documents\\dl\\tft\\.venv\\lib\\site-packages (from transformers->-r requirements.txt (line 8)) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\luan\\documents\\dl\\tft\\.venv\\lib\\site-packages (from transformers->-r requirements.txt (line 8)) (0.20.1)\n",
      "Collecting pyrate-limiter<3.0 (from requests_ratelimiter->-r requirements.txt (line 9))\n",
      "  Using cached pyrate_limiter-2.10.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4>=4.11.1->yfinance->-r requirements.txt (line 5))\n",
      "  Using cached soupsieve-2.6-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: six>=1.9 in c:\\users\\luan\\documents\\dl\\tft\\.venv\\lib\\site-packages (from html5lib>=1.1->yfinance->-r requirements.txt (line 5)) (1.16.0)\n",
      "Collecting webencodings (from html5lib>=1.1->yfinance->-r requirements.txt (line 5))\n",
      "  Using cached webencodings-0.5.1-py2.py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\luan\\documents\\dl\\tft\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers->-r requirements.txt (line 8)) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\luan\\documents\\dl\\tft\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers->-r requirements.txt (line 8)) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\luan\\documents\\dl\\tft\\.venv\\lib\\site-packages (from requests->kagglehub->-r requirements.txt (line 3)) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\luan\\documents\\dl\\tft\\.venv\\lib\\site-packages (from requests->kagglehub->-r requirements.txt (line 3)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\luan\\documents\\dl\\tft\\.venv\\lib\\site-packages (from requests->kagglehub->-r requirements.txt (line 3)) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\luan\\documents\\dl\\tft\\.venv\\lib\\site-packages (from requests->kagglehub->-r requirements.txt (line 3)) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\luan\\documents\\dl\\tft\\.venv\\lib\\site-packages (from tqdm->kagglehub->-r requirements.txt (line 3)) (0.4.6)\n",
      "Using cached yfinance-0.2.50-py2.py3-none-any.whl (102 kB)\n",
      "Downloading scikit_learn-1.6.0-cp311-cp311-win_amd64.whl (11.1 MB)\n",
      "   ---------------------------------------- 0.0/11.1 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 2.1/11.1 MB 11.8 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 4.5/11.1 MB 11.2 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 6.3/11.1 MB 11.0 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 7.9/11.1 MB 9.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.0/11.1 MB 10.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.1/11.1 MB 10.4 MB/s eta 0:00:00\n",
      "Using cached requests_ratelimiter-0.7.0-py3-none-any.whl (9.3 kB)\n",
      "Using cached beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "Using cached frozendict-2.4.6-py311-none-any.whl (16 kB)\n",
      "Using cached html5lib-1.1-py2.py3-none-any.whl (112 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached lxml-5.3.0-cp311-cp311-win_amd64.whl (3.8 MB)\n",
      "Using cached multitasking-0.0.11-py3-none-any.whl (8.5 kB)\n",
      "Using cached pyrate_limiter-2.10.0-py3-none-any.whl (16 kB)\n",
      "Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Using cached soupsieve-2.6-py3-none-any.whl (36 kB)\n",
      "Using cached webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Installing collected packages: webencodings, peewee, multitasking, threadpoolctl, soupsieve, pyrate-limiter, lxml, joblib, html5lib, frozendict, scikit-learn, requests_ratelimiter, beautifulsoup4, yfinance, pandas_ta\n",
      "Successfully installed beautifulsoup4-4.12.3 frozendict-2.4.6 html5lib-1.1 joblib-1.4.2 lxml-5.3.0 multitasking-0.0.11 pandas_ta-0.3.14b0 peewee-3.17.8 pyrate-limiter-2.10.0 requests_ratelimiter-0.7.0 scikit-learn-1.6.0 soupsieve-2.6 threadpoolctl-3.5.0 webencodings-0.5.1 yfinance-0.2.50\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from data_util import get_data_from_kaggle, restructure_date_information, get_static_df, one_label_scale_static_df, scale_stock_data\n",
    "from constant import Constant\n",
    "import pandas as pd\n",
    "from torch import optim\n",
    "from TFT import TFT_embedding, TFT, QuantilesLoss\n",
    "from data import TFT_Dataset\n",
    "from torch.utils.data import DataLoader ,random_split\n",
    "from data_util import get_feature_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TFT.tft.TFT"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<constant.Constant at 0x2ab8dd0e950>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constant = Constant()\n",
    "constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "market = 'sp500' # [forbes2000, nasdaq, nyse, sp500]\n",
    "year = '2017'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stock = get_data_from_kaggle(market = 'sp500', start_date = f'01-01-{year}')\n",
    "# generating static df can take a while because there is a rate limit with yfinance api\n",
    "# static = get_static_df(stock,  constant.static_variables)\n",
    "\n",
    "# stock.to_csv(f'dataset/sp500_{year}.csv', index=False)\n",
    "# static.to_csv(f\"dataset/sp500_{year}_static.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock = pd.read_csv(f'dataset/sp500_{year}.csv')\n",
    "static = pd.read_csv(f\"dataset/sp500_{year}_static.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Volume</th>\n",
       "      <th>High</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adjusted Close</th>\n",
       "      <th>Stock Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-03</td>\n",
       "      <td>45.740002</td>\n",
       "      <td>45.930000</td>\n",
       "      <td>1739600.0</td>\n",
       "      <td>46.750000</td>\n",
       "      <td>46.490002</td>\n",
       "      <td>44.433620</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>46.820000</td>\n",
       "      <td>46.930000</td>\n",
       "      <td>1821300.0</td>\n",
       "      <td>47.380001</td>\n",
       "      <td>47.099998</td>\n",
       "      <td>45.016628</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-05</td>\n",
       "      <td>46.360001</td>\n",
       "      <td>47.049999</td>\n",
       "      <td>1503700.0</td>\n",
       "      <td>47.070000</td>\n",
       "      <td>46.540001</td>\n",
       "      <td>44.481392</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01-06</td>\n",
       "      <td>46.560001</td>\n",
       "      <td>46.630001</td>\n",
       "      <td>2883400.0</td>\n",
       "      <td>48.070000</td>\n",
       "      <td>47.990002</td>\n",
       "      <td>45.867260</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-01-09</td>\n",
       "      <td>47.910000</td>\n",
       "      <td>48.009998</td>\n",
       "      <td>2575300.0</td>\n",
       "      <td>48.560001</td>\n",
       "      <td>48.139999</td>\n",
       "      <td>46.010628</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605210</th>\n",
       "      <td>2022-12-06</td>\n",
       "      <td>152.089996</td>\n",
       "      <td>154.220001</td>\n",
       "      <td>1964800.0</td>\n",
       "      <td>155.500000</td>\n",
       "      <td>153.050003</td>\n",
       "      <td>153.050003</td>\n",
       "      <td>ZTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605211</th>\n",
       "      <td>2022-12-07</td>\n",
       "      <td>149.380005</td>\n",
       "      <td>152.960007</td>\n",
       "      <td>2444100.0</td>\n",
       "      <td>153.789993</td>\n",
       "      <td>150.250000</td>\n",
       "      <td>150.250000</td>\n",
       "      <td>ZTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605212</th>\n",
       "      <td>2022-12-08</td>\n",
       "      <td>149.199997</td>\n",
       "      <td>150.529999</td>\n",
       "      <td>2267500.0</td>\n",
       "      <td>154.350006</td>\n",
       "      <td>153.679993</td>\n",
       "      <td>153.679993</td>\n",
       "      <td>ZTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605213</th>\n",
       "      <td>2022-12-09</td>\n",
       "      <td>152.740005</td>\n",
       "      <td>153.940002</td>\n",
       "      <td>3274900.0</td>\n",
       "      <td>156.330002</td>\n",
       "      <td>153.389999</td>\n",
       "      <td>153.389999</td>\n",
       "      <td>ZTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605214</th>\n",
       "      <td>2022-12-12</td>\n",
       "      <td>152.970001</td>\n",
       "      <td>154.070007</td>\n",
       "      <td>301135.0</td>\n",
       "      <td>154.470001</td>\n",
       "      <td>153.625000</td>\n",
       "      <td>153.625000</td>\n",
       "      <td>ZTS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>605215 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Date         Low        Open     Volume        High       Close  \\\n",
       "0       2017-01-03   45.740002   45.930000  1739600.0   46.750000   46.490002   \n",
       "1       2017-01-04   46.820000   46.930000  1821300.0   47.380001   47.099998   \n",
       "2       2017-01-05   46.360001   47.049999  1503700.0   47.070000   46.540001   \n",
       "3       2017-01-06   46.560001   46.630001  2883400.0   48.070000   47.990002   \n",
       "4       2017-01-09   47.910000   48.009998  2575300.0   48.560001   48.139999   \n",
       "...            ...         ...         ...        ...         ...         ...   \n",
       "605210  2022-12-06  152.089996  154.220001  1964800.0  155.500000  153.050003   \n",
       "605211  2022-12-07  149.380005  152.960007  2444100.0  153.789993  150.250000   \n",
       "605212  2022-12-08  149.199997  150.529999  2267500.0  154.350006  153.679993   \n",
       "605213  2022-12-09  152.740005  153.940002  3274900.0  156.330002  153.389999   \n",
       "605214  2022-12-12  152.970001  154.070007   301135.0  154.470001  153.625000   \n",
       "\n",
       "        Adjusted Close Stock Name  \n",
       "0            44.433620          A  \n",
       "1            45.016628          A  \n",
       "2            44.481392          A  \n",
       "3            45.867260          A  \n",
       "4            46.010628          A  \n",
       "...                ...        ...  \n",
       "605210      153.050003        ZTS  \n",
       "605211      150.250000        ZTS  \n",
       "605212      153.679993        ZTS  \n",
       "605213      153.389999        ZTS  \n",
       "605214      153.625000        ZTS  \n",
       "\n",
       "[605215 rows x 8 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>industry</th>\n",
       "      <th>sector</th>\n",
       "      <th>country</th>\n",
       "      <th>beta</th>\n",
       "      <th>marketCap</th>\n",
       "      <th>bookValue</th>\n",
       "      <th>dividendRate</th>\n",
       "      <th>dividendYield</th>\n",
       "      <th>fiveYearAvgDividendYield</th>\n",
       "      <th>debtToEquity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <td>Diagnostics &amp; Research</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>United States</td>\n",
       "      <td>1.072</td>\n",
       "      <td>39891210240</td>\n",
       "      <td>20.530</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.0074</td>\n",
       "      <td>0.66</td>\n",
       "      <td>52.787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAL</th>\n",
       "      <td>Airlines</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>United States</td>\n",
       "      <td>1.399</td>\n",
       "      <td>9680030720</td>\n",
       "      <td>-7.387</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.27</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAP</th>\n",
       "      <td>Specialty Retail</td>\n",
       "      <td>Consumer Cyclical</td>\n",
       "      <td>United States</td>\n",
       "      <td>1.194</td>\n",
       "      <td>2744477440</td>\n",
       "      <td>42.340</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0243</td>\n",
       "      <td>2.44</td>\n",
       "      <td>171.801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAPL</th>\n",
       "      <td>Consumer Electronics</td>\n",
       "      <td>Technology</td>\n",
       "      <td>United States</td>\n",
       "      <td>1.240</td>\n",
       "      <td>3622499778560</td>\n",
       "      <td>3.767</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0042</td>\n",
       "      <td>0.62</td>\n",
       "      <td>209.059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABBV</th>\n",
       "      <td>Drug Manufacturers - General</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>United States</td>\n",
       "      <td>0.613</td>\n",
       "      <td>322113306624</td>\n",
       "      <td>3.413</td>\n",
       "      <td>6.56</td>\n",
       "      <td>0.0359</td>\n",
       "      <td>4.13</td>\n",
       "      <td>1174.815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XYL</th>\n",
       "      <td>Specialty Industrial Machinery</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>United States</td>\n",
       "      <td>1.050</td>\n",
       "      <td>30846726144</td>\n",
       "      <td>43.593</td>\n",
       "      <td>1.44</td>\n",
       "      <td>0.0114</td>\n",
       "      <td>1.16</td>\n",
       "      <td>19.840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YUM</th>\n",
       "      <td>Restaurants</td>\n",
       "      <td>Consumer Cyclical</td>\n",
       "      <td>United States</td>\n",
       "      <td>1.095</td>\n",
       "      <td>38885851136</td>\n",
       "      <td>-27.407</td>\n",
       "      <td>2.68</td>\n",
       "      <td>0.0193</td>\n",
       "      <td>1.81</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZBH</th>\n",
       "      <td>Medical Devices</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>United States</td>\n",
       "      <td>1.021</td>\n",
       "      <td>21979760640</td>\n",
       "      <td>61.997</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.0086</td>\n",
       "      <td>0.75</td>\n",
       "      <td>53.611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZION</th>\n",
       "      <td>Banks - Regional</td>\n",
       "      <td>Financial Services</td>\n",
       "      <td>United States</td>\n",
       "      <td>1.060</td>\n",
       "      <td>8874271744</td>\n",
       "      <td>40.251</td>\n",
       "      <td>1.72</td>\n",
       "      <td>0.0284</td>\n",
       "      <td>3.47</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZTS</th>\n",
       "      <td>Drug Manufacturers - Specialty &amp; Generic</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>United States</td>\n",
       "      <td>0.896</td>\n",
       "      <td>80394125312</td>\n",
       "      <td>11.591</td>\n",
       "      <td>1.73</td>\n",
       "      <td>0.0099</td>\n",
       "      <td>0.68</td>\n",
       "      <td>129.442</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>408 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      industry              sector  \\\n",
       "A                       Diagnostics & Research          Healthcare   \n",
       "AAL                                   Airlines         Industrials   \n",
       "AAP                           Specialty Retail   Consumer Cyclical   \n",
       "AAPL                      Consumer Electronics          Technology   \n",
       "ABBV              Drug Manufacturers - General          Healthcare   \n",
       "...                                        ...                 ...   \n",
       "XYL             Specialty Industrial Machinery         Industrials   \n",
       "YUM                                Restaurants   Consumer Cyclical   \n",
       "ZBH                            Medical Devices          Healthcare   \n",
       "ZION                          Banks - Regional  Financial Services   \n",
       "ZTS   Drug Manufacturers - Specialty & Generic          Healthcare   \n",
       "\n",
       "            country   beta      marketCap  bookValue  dividendRate  \\\n",
       "A     United States  1.072    39891210240     20.530          0.99   \n",
       "AAL   United States  1.399     9680030720     -7.387          0.00   \n",
       "AAP   United States  1.194     2744477440     42.340          1.00   \n",
       "AAPL  United States  1.240  3622499778560      3.767          1.00   \n",
       "ABBV  United States  0.613   322113306624      3.413          6.56   \n",
       "...             ...    ...            ...        ...           ...   \n",
       "XYL   United States  1.050    30846726144     43.593          1.44   \n",
       "YUM   United States  1.095    38885851136    -27.407          2.68   \n",
       "ZBH   United States  1.021    21979760640     61.997          0.96   \n",
       "ZION  United States  1.060     8874271744     40.251          1.72   \n",
       "ZTS   United States  0.896    80394125312     11.591          1.73   \n",
       "\n",
       "      dividendYield  fiveYearAvgDividendYield  debtToEquity  \n",
       "A            0.0074                      0.66        52.787  \n",
       "AAL          0.0000                      1.27         0.000  \n",
       "AAP          0.0243                      2.44       171.801  \n",
       "AAPL         0.0042                      0.62       209.059  \n",
       "ABBV         0.0359                      4.13      1174.815  \n",
       "...             ...                       ...           ...  \n",
       "XYL          0.0114                      1.16        19.840  \n",
       "YUM          0.0193                      1.81         0.000  \n",
       "ZBH          0.0086                      0.75        53.611  \n",
       "ZION         0.0284                      3.47         0.000  \n",
       "ZTS          0.0099                      0.68       129.442  \n",
       "\n",
       "[408 rows x 10 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Consumer Electronics', 'Technology', 'United States',\n",
       "       np.float64(1.24), np.int64(3622499778560), np.float64(3.767),\n",
       "       np.float64(1.0), np.float64(0.0042), np.float64(0.62),\n",
       "       np.float64(209.059)], dtype=object)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "static.loc[\"AAPL\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_tickers = static.index\n",
    "stock = stock[stock['Stock Name'].isin(valid_tickers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock = stock.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Volume</th>\n",
       "      <th>High</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adjusted Close</th>\n",
       "      <th>Stock Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.108231</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>0.008217</td>\n",
       "      <td>0.007499</td>\n",
       "      <td>0.113930</td>\n",
       "      <td>0.004743</td>\n",
       "      <td>0.004594</td>\n",
       "      <td>0.004368</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-05</td>\n",
       "      <td>0.004717</td>\n",
       "      <td>0.008399</td>\n",
       "      <td>0.091776</td>\n",
       "      <td>0.002409</td>\n",
       "      <td>0.000377</td>\n",
       "      <td>0.000358</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01-06</td>\n",
       "      <td>0.006239</td>\n",
       "      <td>0.005249</td>\n",
       "      <td>0.188017</td>\n",
       "      <td>0.009938</td>\n",
       "      <td>0.011296</td>\n",
       "      <td>0.010740</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-01-09</td>\n",
       "      <td>0.016511</td>\n",
       "      <td>0.015598</td>\n",
       "      <td>0.166526</td>\n",
       "      <td>0.013627</td>\n",
       "      <td>0.012426</td>\n",
       "      <td>0.011814</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605210</th>\n",
       "      <td>2022-12-06</td>\n",
       "      <td>0.518709</td>\n",
       "      <td>0.522872</td>\n",
       "      <td>0.132240</td>\n",
       "      <td>0.522823</td>\n",
       "      <td>0.516862</td>\n",
       "      <td>0.526255</td>\n",
       "      <td>ZTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605211</th>\n",
       "      <td>2022-12-07</td>\n",
       "      <td>0.504664</td>\n",
       "      <td>0.516396</td>\n",
       "      <td>0.170338</td>\n",
       "      <td>0.514121</td>\n",
       "      <td>0.502468</td>\n",
       "      <td>0.511865</td>\n",
       "      <td>ZTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605212</th>\n",
       "      <td>2022-12-08</td>\n",
       "      <td>0.503731</td>\n",
       "      <td>0.503906</td>\n",
       "      <td>0.156301</td>\n",
       "      <td>0.516971</td>\n",
       "      <td>0.520101</td>\n",
       "      <td>0.529492</td>\n",
       "      <td>ZTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605213</th>\n",
       "      <td>2022-12-09</td>\n",
       "      <td>0.522077</td>\n",
       "      <td>0.521433</td>\n",
       "      <td>0.236376</td>\n",
       "      <td>0.527047</td>\n",
       "      <td>0.518610</td>\n",
       "      <td>0.528002</td>\n",
       "      <td>ZTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605214</th>\n",
       "      <td>2022-12-12</td>\n",
       "      <td>0.523269</td>\n",
       "      <td>0.522101</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.517582</td>\n",
       "      <td>0.519818</td>\n",
       "      <td>0.529210</td>\n",
       "      <td>ZTS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>605215 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Date       Low      Open    Volume      High     Close  \\\n",
       "0       2017-01-03  0.000000  0.000000  0.108231  0.000000  0.000000   \n",
       "1       2017-01-04  0.008217  0.007499  0.113930  0.004743  0.004594   \n",
       "2       2017-01-05  0.004717  0.008399  0.091776  0.002409  0.000377   \n",
       "3       2017-01-06  0.006239  0.005249  0.188017  0.009938  0.011296   \n",
       "4       2017-01-09  0.016511  0.015598  0.166526  0.013627  0.012426   \n",
       "...            ...       ...       ...       ...       ...       ...   \n",
       "605210  2022-12-06  0.518709  0.522872  0.132240  0.522823  0.516862   \n",
       "605211  2022-12-07  0.504664  0.516396  0.170338  0.514121  0.502468   \n",
       "605212  2022-12-08  0.503731  0.503906  0.156301  0.516971  0.520101   \n",
       "605213  2022-12-09  0.522077  0.521433  0.236376  0.527047  0.518610   \n",
       "605214  2022-12-12  0.523269  0.522101  0.000000  0.517582  0.519818   \n",
       "\n",
       "        Adjusted Close Stock Name  \n",
       "0             0.000000          A  \n",
       "1             0.004368          A  \n",
       "2             0.000358          A  \n",
       "3             0.010740          A  \n",
       "4             0.011814          A  \n",
       "...                ...        ...  \n",
       "605210        0.526255        ZTS  \n",
       "605211        0.511865        ZTS  \n",
       "605212        0.529492        ZTS  \n",
       "605213        0.528002        ZTS  \n",
       "605214        0.529210        ZTS  \n",
       "\n",
       "[605215 rows x 8 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_scaled, stock_scalar = scale_stock_data(stock, constant.columns_to_scale)\n",
    "stock_scaled "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Volume</th>\n",
       "      <th>High</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adjusted Close</th>\n",
       "      <th>Stock Name</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Day of The Week</th>\n",
       "      <th>Week of The Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.108231</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.008217</td>\n",
       "      <td>0.007499</td>\n",
       "      <td>0.113930</td>\n",
       "      <td>0.004743</td>\n",
       "      <td>0.004594</td>\n",
       "      <td>0.004368</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.004717</td>\n",
       "      <td>0.008399</td>\n",
       "      <td>0.091776</td>\n",
       "      <td>0.002409</td>\n",
       "      <td>0.000377</td>\n",
       "      <td>0.000358</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.006239</td>\n",
       "      <td>0.005249</td>\n",
       "      <td>0.188017</td>\n",
       "      <td>0.009938</td>\n",
       "      <td>0.011296</td>\n",
       "      <td>0.010740</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.016511</td>\n",
       "      <td>0.015598</td>\n",
       "      <td>0.166526</td>\n",
       "      <td>0.013627</td>\n",
       "      <td>0.012426</td>\n",
       "      <td>0.011814</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605210</th>\n",
       "      <td>0.518709</td>\n",
       "      <td>0.522872</td>\n",
       "      <td>0.132240</td>\n",
       "      <td>0.522823</td>\n",
       "      <td>0.516862</td>\n",
       "      <td>0.526255</td>\n",
       "      <td>ZTS</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605211</th>\n",
       "      <td>0.504664</td>\n",
       "      <td>0.516396</td>\n",
       "      <td>0.170338</td>\n",
       "      <td>0.514121</td>\n",
       "      <td>0.502468</td>\n",
       "      <td>0.511865</td>\n",
       "      <td>ZTS</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605212</th>\n",
       "      <td>0.503731</td>\n",
       "      <td>0.503906</td>\n",
       "      <td>0.156301</td>\n",
       "      <td>0.516971</td>\n",
       "      <td>0.520101</td>\n",
       "      <td>0.529492</td>\n",
       "      <td>ZTS</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605213</th>\n",
       "      <td>0.522077</td>\n",
       "      <td>0.521433</td>\n",
       "      <td>0.236376</td>\n",
       "      <td>0.527047</td>\n",
       "      <td>0.518610</td>\n",
       "      <td>0.528002</td>\n",
       "      <td>ZTS</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605214</th>\n",
       "      <td>0.523269</td>\n",
       "      <td>0.522101</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.517582</td>\n",
       "      <td>0.519818</td>\n",
       "      <td>0.529210</td>\n",
       "      <td>ZTS</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>605215 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Low      Open    Volume      High     Close  Adjusted Close  \\\n",
       "0       0.000000  0.000000  0.108231  0.000000  0.000000        0.000000   \n",
       "1       0.008217  0.007499  0.113930  0.004743  0.004594        0.004368   \n",
       "2       0.004717  0.008399  0.091776  0.002409  0.000377        0.000358   \n",
       "3       0.006239  0.005249  0.188017  0.009938  0.011296        0.010740   \n",
       "4       0.016511  0.015598  0.166526  0.013627  0.012426        0.011814   \n",
       "...          ...       ...       ...       ...       ...             ...   \n",
       "605210  0.518709  0.522872  0.132240  0.522823  0.516862        0.526255   \n",
       "605211  0.504664  0.516396  0.170338  0.514121  0.502468        0.511865   \n",
       "605212  0.503731  0.503906  0.156301  0.516971  0.520101        0.529492   \n",
       "605213  0.522077  0.521433  0.236376  0.527047  0.518610        0.528002   \n",
       "605214  0.523269  0.522101  0.000000  0.517582  0.519818        0.529210   \n",
       "\n",
       "       Stock Name  Month  Day  Day of The Week  Week of The Year  \n",
       "0               A      0    2                1                 0  \n",
       "1               A      0    3                2                 0  \n",
       "2               A      0    4                3                 0  \n",
       "3               A      0    5                4                 0  \n",
       "4               A      0    8                0                 1  \n",
       "...           ...    ...  ...              ...               ...  \n",
       "605210        ZTS     11    5                1                48  \n",
       "605211        ZTS     11    6                2                48  \n",
       "605212        ZTS     11    7                3                48  \n",
       "605213        ZTS     11    8                4                48  \n",
       "605214        ZTS     11   11                0                49  \n",
       "\n",
       "[605215 rows x 11 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_df = restructure_date_information(stock_scaled)\n",
    "processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_df = one_label_scale_static_df(static,constant.static_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_cat_feature_num_list, history_cont_feature_num = get_feature_length(processed_df, constant.feature_variables)\n",
    "static_cat_feature_num_list , static_cont_feature_num  = get_feature_length(static, constant.static_variables)\n",
    "future_cat_feature_num_list , _                        = get_feature_length(processed_df, constant.future_feature)\n",
    "_                           , prediction_con           = get_feature_length(processed_df, constant.prediction_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([12, 31, 5, 53], 4)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_cat_feature_num_list, history_cont_feature_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_length = 90\n",
    "prediction_length = 15\n",
    "hidden_size = 128\n",
    "batch_size = 128\n",
    "dropout = .2\n",
    "num_head = 4\n",
    "lr= 0.0001\n",
    "momentum=0.9\n",
    "num_epochs = 10\n",
    "max_grad_norm = 1\n",
    "split_ratio = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "tft_Dataset = TFT_Dataset(stock_df= processed_df , \n",
    "                        static_df=static_df,\n",
    "                        constant_variable=constant, \n",
    "                        history_length= history_length, \n",
    "                        prediction_length= prediction_length, \n",
    "                        device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * tft_Dataset.__len__())\n",
    "test_size = len(tft_Dataset) - train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = random_split(tft_Dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size,shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import vit_b_16, VisionTransformer\n",
    "import torch.nn as nn\n",
    "\n",
    "# Load the pretrained ViT model\n",
    "model = vit_b_16(pretrained=True)  # or use vit_l_16 for a larger model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = model.heads[0].in_features # Get the input size of the last layer\n",
    "num_classes = 10  # Replace with the number of classes for your task\n",
    "\n",
    "# Replace the last layer\n",
    "model.heads = nn.Linear(768, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 3, 224, 224]) cuda:0\n"
     ]
    }
   ],
   "source": [
    "count =0\n",
    "for static_cont_input, static_cat_input,history_cont_input, history_cat_input, future_input, prediction,img, prediction_classification in train_loader:\n",
    "    break\n",
    "print(img.size(), img.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TFT(\n",
       "  (tft_embed): TFT_embedding(\n",
       "    (static_cont): Linear(in_features=7, out_features=128, bias=True)\n",
       "    (static_cat): ModuleList(\n",
       "      (0): Embedding(108, 128)\n",
       "      (1-2): 2 x Embedding(12, 128)\n",
       "    )\n",
       "    (history_cont): Linear(in_features=4, out_features=128, bias=True)\n",
       "    (history_cat): ModuleList(\n",
       "      (0): Embedding(12, 128)\n",
       "      (1): Embedding(31, 128)\n",
       "      (2): Embedding(5, 128)\n",
       "      (3): Embedding(53, 128)\n",
       "    )\n",
       "    (future_feature): ModuleList(\n",
       "      (0): Embedding(12, 128)\n",
       "      (1): Embedding(31, 128)\n",
       "      (2): Embedding(5, 128)\n",
       "      (3): Embedding(53, 128)\n",
       "    )\n",
       "  )\n",
       "  (cs): VariationSelection(\n",
       "    (group_GRN): GRN(\n",
       "      (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "      (GLU): GLU(\n",
       "        (linear4): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (linear5): Linear(in_features=128, out_features=512, bias=True)\n",
       "      )\n",
       "      (droput): Dropout(p=0.0, inplace=False)\n",
       "      (layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (linear_out): Linear(in_features=512, out_features=4, bias=True)\n",
       "    )\n",
       "    (individual_GRN): ModuleList(\n",
       "      (0-3): 4 x GRN(\n",
       "        (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (GLU): GLU(\n",
       "          (linear4): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear5): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (droput): Dropout(p=0.0, inplace=False)\n",
       "        (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ce): VariationSelection(\n",
       "    (group_GRN): GRN(\n",
       "      (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "      (GLU): GLU(\n",
       "        (linear4): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (linear5): Linear(in_features=128, out_features=512, bias=True)\n",
       "      )\n",
       "      (droput): Dropout(p=0.0, inplace=False)\n",
       "      (layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (linear_out): Linear(in_features=512, out_features=4, bias=True)\n",
       "    )\n",
       "    (individual_GRN): ModuleList(\n",
       "      (0-3): 4 x GRN(\n",
       "        (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (GLU): GLU(\n",
       "          (linear4): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear5): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (droput): Dropout(p=0.0, inplace=False)\n",
       "        (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cc): VariationSelection(\n",
       "    (group_GRN): GRN(\n",
       "      (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "      (GLU): GLU(\n",
       "        (linear4): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (linear5): Linear(in_features=128, out_features=512, bias=True)\n",
       "      )\n",
       "      (droput): Dropout(p=0.0, inplace=False)\n",
       "      (layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (linear_out): Linear(in_features=512, out_features=4, bias=True)\n",
       "    )\n",
       "    (individual_GRN): ModuleList(\n",
       "      (0-3): 4 x GRN(\n",
       "        (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (GLU): GLU(\n",
       "          (linear4): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear5): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (droput): Dropout(p=0.0, inplace=False)\n",
       "        (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ch): VariationSelection(\n",
       "    (group_GRN): GRN(\n",
       "      (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "      (GLU): GLU(\n",
       "        (linear4): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (linear5): Linear(in_features=128, out_features=512, bias=True)\n",
       "      )\n",
       "      (droput): Dropout(p=0.0, inplace=False)\n",
       "      (layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (linear_out): Linear(in_features=512, out_features=4, bias=True)\n",
       "    )\n",
       "    (individual_GRN): ModuleList(\n",
       "      (0-3): 4 x GRN(\n",
       "        (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (GLU): GLU(\n",
       "          (linear4): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear5): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (droput): Dropout(p=0.0, inplace=False)\n",
       "        (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (history_variation): VariationSelection(\n",
       "    (group_GRN): GRN(\n",
       "      (linear1): Linear(in_features=640, out_features=640, bias=True)\n",
       "      (linear2): Linear(in_features=640, out_features=128, bias=True)\n",
       "      (GLU): GLU(\n",
       "        (linear4): Linear(in_features=128, out_features=640, bias=True)\n",
       "        (linear5): Linear(in_features=128, out_features=640, bias=True)\n",
       "      )\n",
       "      (droput): Dropout(p=0.0, inplace=False)\n",
       "      (layernorm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (linear_out): Linear(in_features=640, out_features=5, bias=True)\n",
       "      (linear_conext): Linear(in_features=128, out_features=640, bias=False)\n",
       "    )\n",
       "    (individual_GRN): ModuleList(\n",
       "      (0-4): 5 x GRN(\n",
       "        (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (GLU): GLU(\n",
       "          (linear4): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear5): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (droput): Dropout(p=0.0, inplace=False)\n",
       "        (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (future_variation): VariationSelection(\n",
       "    (group_GRN): GRN(\n",
       "      (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "      (GLU): GLU(\n",
       "        (linear4): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (linear5): Linear(in_features=128, out_features=512, bias=True)\n",
       "      )\n",
       "      (droput): Dropout(p=0.0, inplace=False)\n",
       "      (layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (linear_out): Linear(in_features=512, out_features=4, bias=True)\n",
       "      (linear_conext): Linear(in_features=128, out_features=512, bias=False)\n",
       "    )\n",
       "    (individual_GRN): ModuleList(\n",
       "      (0-3): 4 x GRN(\n",
       "        (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (GLU): GLU(\n",
       "          (linear4): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear5): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (droput): Dropout(p=0.0, inplace=False)\n",
       "        (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (gate_add_norm_history): Gate_Add_Norm(\n",
       "    (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (GLU): GLU(\n",
       "      (linear4): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (linear5): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (gate_add_norm_future): Gate_Add_Norm(\n",
       "    (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (GLU): GLU(\n",
       "      (linear4): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (linear5): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (GRN): GRN(\n",
       "    (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (GLU): GLU(\n",
       "      (linear4): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (linear5): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (droput): Dropout(p=0.0, inplace=False)\n",
       "    (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (linear_conext): Linear(in_features=128, out_features=128, bias=False)\n",
       "  )\n",
       "  (history_lstm): LSTM(128, 128, batch_first=True)\n",
       "  (future_lstm): LSTM(128, 128, batch_first=True)\n",
       "  (history_layernorm): LayerNorm((128,), eps=0.2, elementwise_affine=True)\n",
       "  (future_layernorm): LayerNorm((128,), eps=0.2, elementwise_affine=True)\n",
       "  (InterpAttention): InterpretableMultiHeadAttention(\n",
       "    (q_linear): ModuleList(\n",
       "      (0-3): 4 x Linear(in_features=128, out_features=32, bias=True)\n",
       "    )\n",
       "    (k_linear): ModuleList(\n",
       "      (0-3): 4 x Linear(in_features=128, out_features=32, bias=True)\n",
       "    )\n",
       "    (v_linear): Linear(in_features=128, out_features=105, bias=True)\n",
       "  )\n",
       "  (attention_layernorm): LayerNorm((105,), eps=0.2, elementwise_affine=True)\n",
       "  (gate_add_norm_attention): Gate_Add_Norm(\n",
       "    (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (GLU): GLU(\n",
       "      (linear4): Linear(in_features=105, out_features=128, bias=True)\n",
       "      (linear5): Linear(in_features=105, out_features=128, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (attention_GRN): GRN(\n",
       "    (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (GLU): GLU(\n",
       "      (linear4): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (linear5): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (droput): Dropout(p=0.0, inplace=False)\n",
       "    (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (gate_add_norm_last): Gate_Add_Norm(\n",
       "    (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (GLU): GLU(\n",
       "      (linear4): Linear(in_features=15, out_features=128, bias=True)\n",
       "      (linear5): Linear(in_features=15, out_features=128, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (GLU): GLU(\n",
       "    (linear4): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (linear5): Linear(in_features=128, out_features=128, bias=True)\n",
       "  )\n",
       "  (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  (output_linear): Linear(in_features=128, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TFT(static_cat_feature_num_list= static_cat_feature_num_list,\n",
    "            static_cont_feature_num=static_cont_feature_num,\n",
    "            history_cat_feature_num_list= history_cat_feature_num_list,\n",
    "            history_cont_feature_num=history_cont_feature_num,\n",
    "            future_cat_feature_num_list=future_cat_feature_num_list,\n",
    "            history_len = history_length,\n",
    "            future_len = prediction_length,\n",
    "            dropout= dropout,\n",
    "            num_head = num_head,\n",
    "            hidden_size = hidden_size,\n",
    "            device = device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (conv_proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "  (encoder): Encoder(\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): Sequential(\n",
       "      (encoder_layer_0): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_1): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_2): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_3): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_4): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_5): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_6): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_7): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_8): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_9): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_10): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_11): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (heads): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = QuantilesLoss( device=device)\n",
    "loss_function.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (conv_proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "  (encoder): Encoder(\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): Sequential(\n",
       "      (encoder_layer_0): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_1): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_2): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_3): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_4): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_5): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_6): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_7): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_8): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_9): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_10): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_11): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (heads): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 3, 224, 224])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0159,  0.0473],\n",
       "        [ 0.1473, -0.0058],\n",
       "        [ 0.1033,  0.2388],\n",
       "        [ 0.0826,  0.3620],\n",
       "        [ 0.1845, -0.1964],\n",
       "        [ 0.3641, -0.0635],\n",
       "        [ 0.2068,  0.0936],\n",
       "        [ 0.3073,  0.0435],\n",
       "        [ 0.1700, -0.0080],\n",
       "        [ 0.1270,  0.5614],\n",
       "        [ 0.3069,  0.0672],\n",
       "        [ 0.2406,  0.2316],\n",
       "        [ 0.2629,  0.2016],\n",
       "        [ 0.0464,  0.2424],\n",
       "        [ 0.0166,  0.4956],\n",
       "        [ 0.2094,  0.3031],\n",
       "        [ 0.1810,  0.2033],\n",
       "        [ 0.1719,  0.0646],\n",
       "        [-0.0116, -0.2902],\n",
       "        [ 0.1338, -0.1802],\n",
       "        [ 0.1177,  0.2474],\n",
       "        [ 0.1268,  0.1247],\n",
       "        [ 0.0192,  0.2808],\n",
       "        [ 0.3004,  0.0154],\n",
       "        [ 0.1246,  0.1224],\n",
       "        [ 0.1602,  0.3170],\n",
       "        [ 0.1167,  0.3706],\n",
       "        [ 0.1415,  0.0091],\n",
       "        [ 0.1387,  0.3158],\n",
       "        [ 0.2211,  0.0386],\n",
       "        [ 0.1667,  0.3816],\n",
       "        [ 0.1782,  0.0659],\n",
       "        [ 0.2774,  0.0563],\n",
       "        [ 0.2977,  0.3071],\n",
       "        [ 0.0868,  0.1283],\n",
       "        [ 0.1708,  0.1407],\n",
       "        [-0.0723,  0.2910],\n",
       "        [ 0.1415,  0.2003],\n",
       "        [ 0.2536,  0.3151],\n",
       "        [ 0.2137,  0.1751],\n",
       "        [ 0.2139, -0.0106],\n",
       "        [ 0.2562,  0.0583],\n",
       "        [ 0.2525,  0.2491],\n",
       "        [ 0.2298,  0.2876],\n",
       "        [ 0.1232,  0.1402],\n",
       "        [ 0.0523,  0.3466],\n",
       "        [ 0.2122,  0.0117],\n",
       "        [ 0.1468,  0.1554],\n",
       "        [ 0.2511, -0.0526],\n",
       "        [ 0.1746,  0.0131],\n",
       "        [-0.0056,  0.2590],\n",
       "        [ 0.2177,  0.1690],\n",
       "        [ 0.1159,  0.2124],\n",
       "        [ 0.1619, -0.3207],\n",
       "        [ 0.1593,  0.1008],\n",
       "        [ 0.1232,  0.2483],\n",
       "        [ 0.0369,  0.1285],\n",
       "        [ 0.1695, -0.0129],\n",
       "        [ 0.1820,  0.2898],\n",
       "        [ 0.1140,  0.4077],\n",
       "        [ 0.0359,  0.2175],\n",
       "        [-0.1044, -0.1411],\n",
       "        [ 0.1106, -0.1640],\n",
       "        [ 0.2334, -0.0538],\n",
       "        [ 0.2301,  0.2576],\n",
       "        [ 0.2655,  0.4613],\n",
       "        [ 0.2453, -0.2727],\n",
       "        [ 0.2457,  0.3026],\n",
       "        [ 0.2623,  0.0449],\n",
       "        [ 0.2246,  0.2030],\n",
       "        [ 0.1678,  0.0271],\n",
       "        [ 0.1797,  0.2035],\n",
       "        [ 0.2220,  0.0286],\n",
       "        [ 0.1045,  0.2678],\n",
       "        [ 0.1802,  0.1957],\n",
       "        [ 0.2617,  0.0264],\n",
       "        [ 0.1249,  0.0777],\n",
       "        [ 0.2368,  0.1320],\n",
       "        [ 0.0905,  0.4092],\n",
       "        [ 0.0517,  0.0323],\n",
       "        [ 0.1436, -0.2168],\n",
       "        [ 0.1034,  0.3371],\n",
       "        [ 0.2145,  0.2487],\n",
       "        [ 0.1639,  0.1897],\n",
       "        [ 0.1938,  0.3470],\n",
       "        [ 0.1811,  0.0612],\n",
       "        [ 0.2001,  0.1895],\n",
       "        [-0.0481,  0.0747],\n",
       "        [-0.0556,  0.8879],\n",
       "        [ 0.1369,  0.3104],\n",
       "        [ 0.1762,  0.0507],\n",
       "        [ 0.1897, -0.0600],\n",
       "        [ 0.2218, -0.0156],\n",
       "        [ 0.2365, -0.0272],\n",
       "        [ 0.2101,  0.1727],\n",
       "        [ 0.1578,  0.4213],\n",
       "        [ 0.3160,  0.2733],\n",
       "        [ 0.1590,  0.2071],\n",
       "        [ 0.2606, -0.0795],\n",
       "        [ 0.2510,  0.2904],\n",
       "        [ 0.2665,  0.2157],\n",
       "        [ 0.0697,  0.5547],\n",
       "        [ 0.2067,  0.1929],\n",
       "        [ 0.1486,  0.2414],\n",
       "        [ 0.2205, -0.1420],\n",
       "        [ 0.2317,  0.2116],\n",
       "        [ 0.1612,  0.0700],\n",
       "        [ 0.2951,  0.0597],\n",
       "        [ 0.2132,  0.1515],\n",
       "        [ 0.2836,  0.0202],\n",
       "        [ 0.0658,  0.3486],\n",
       "        [ 0.2271,  0.2828],\n",
       "        [ 0.1325, -0.0543],\n",
       "        [ 0.2606, -0.0530],\n",
       "        [ 0.1335,  0.3160],\n",
       "        [ 0.1347,  0.1694],\n",
       "        [ 0.1919,  0.2184],\n",
       "        [ 0.1660, -0.0148],\n",
       "        [ 0.2552,  0.3329],\n",
       "        [ 0.1982,  0.1532],\n",
       "        [ 0.2514,  0.5614],\n",
       "        [ 0.1159,  0.3013],\n",
       "        [ 0.1565,  0.1853],\n",
       "        [ 0.1972, -0.0454],\n",
       "        [ 0.1367,  0.3143],\n",
       "        [ 0.3059,  0.1573],\n",
       "        [ 0.2971,  0.0413],\n",
       "        [ 0.2168, -0.1892]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 296.00 MiB. GPU 0 has a total capacity of 6.00 GiB of which 0 bytes is free. Of the allocated memory 20.34 GiB is allocated by PyTorch, and 126.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[87], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, (static_cont_input, static_cat_input,history_cont_input, history_cat_input, future_input, prediction,  img, prediction_classification) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m----> 5\u001b[0m         predicted \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m      7\u001b[0m         losses \u001b[38;5;241m=\u001b[39m loss_function(predicted \u001b[38;5;241m=\u001b[39m predicted, targets \u001b[38;5;241m=\u001b[39m prediction_classification)\n",
      "File \u001b[1;32mc:\\Users\\Luan\\Documents\\DL\\TFT\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Luan\\Documents\\DL\\TFT\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Luan\\Documents\\DL\\TFT\\.venv\\Lib\\site-packages\\torchvision\\models\\vision_transformer.py:298\u001b[0m, in \u001b[0;36mVisionTransformer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    295\u001b[0m batch_class_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_token\u001b[38;5;241m.\u001b[39mexpand(n, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    296\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([batch_class_token, x], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 298\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    300\u001b[0m \u001b[38;5;66;03m# Classifier \"token\" as used by standard language architectures\u001b[39;00m\n\u001b[0;32m    301\u001b[0m x \u001b[38;5;241m=\u001b[39m x[:, \u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Luan\\Documents\\DL\\TFT\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Luan\\Documents\\DL\\TFT\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Luan\\Documents\\DL\\TFT\\.venv\\Lib\\site-packages\\torchvision\\models\\vision_transformer.py:157\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    155\u001b[0m torch\u001b[38;5;241m.\u001b[39m_assert(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected (batch_size, seq_length, hidden_dim) got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embedding\n\u001b[1;32m--> 157\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\Luan\\Documents\\DL\\TFT\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Luan\\Documents\\DL\\TFT\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Luan\\Documents\\DL\\TFT\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Luan\\Documents\\DL\\TFT\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Luan\\Documents\\DL\\TFT\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Luan\\Documents\\DL\\TFT\\.venv\\Lib\\site-packages\\torchvision\\models\\vision_transformer.py:118\u001b[0m, in \u001b[0;36mEncoderBlock.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[0;32m    117\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_2(x)\n\u001b[1;32m--> 118\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m+\u001b[39m y\n",
      "File \u001b[1;32mc:\\Users\\Luan\\Documents\\DL\\TFT\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Luan\\Documents\\DL\\TFT\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Luan\\Documents\\DL\\TFT\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Luan\\Documents\\DL\\TFT\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Luan\\Documents\\DL\\TFT\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Luan\\Documents\\DL\\TFT\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 296.00 MiB. GPU 0 has a total capacity of 6.00 GiB of which 0 bytes is free. Of the allocated memory 20.34 GiB is allocated by PyTorch, and 126.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "batch_loss = [] \n",
    "for epoch in range(num_epochs):\n",
    "    for i, (static_cont_input, static_cat_input,history_cont_input, history_cat_input, future_input, prediction,  img, prediction_classification) in enumerate(train_loader):\n",
    "        predicted = model(img)\n",
    "        optimizer.zero_grad()\n",
    "        losses = loss_function(predicted = predicted, targets = prediction_classification)\n",
    "        loss = losses.sum()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        if i % 1000 == 0:\n",
    "            print(f'Epoch: {epoch+1}, Batch: {i},  Loss: {loss.item()}')\n",
    "    batch_loss.append(loss.item())\n",
    "    print(\"###########################\")\n",
    "    print(f'Epoch: {epoch+1},  Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f\"vit_model_{year}.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(len(batch_loss))\n",
    "plt.plot(x, batch_loss)\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TFT(static_cat_feature_num_list= static_cat_feature_num_list,\n",
    "            static_cont_feature_num=static_cont_feature_num,\n",
    "            history_cat_feature_num_list= history_cat_feature_num_list,\n",
    "            history_cont_feature_num=history_cont_feature_num,\n",
    "            future_cat_feature_num_list=future_cat_feature_num_list,\n",
    "            history_len = history_length,\n",
    "            future_len = prediction_length,\n",
    "            dropout= dropout,\n",
    "            num_head = num_head,\n",
    "            hidden_size = hidden_size,\n",
    "            device = device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(f'tft_model_{year}.pth', weights_only=True))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "count = 0\n",
    "err = 0\n",
    "with torch.no_grad():\n",
    "    for i, (static_cont_input, static_cat_input,history_cont_input, history_cat_input, future_input, prediction) in enumerate(test_loader):\n",
    "        predicted = model(static_cont_input, static_cat_input,history_cont_input, history_cat_input, future_input)\n",
    "\n",
    "        diff = (prediction.squeeze(-1) - predicted[:,:,1])**2\n",
    "        err += diff.sum()\n",
    "        count += diff.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  0.07693053781986237\n"
     ]
    }
   ],
   "source": [
    "print(\"MSE: \" , (err/count).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'count' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mcount\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'count' is not defined"
     ]
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, hidden_size,n_head,q_seq_len, k_v_seq_len, device):\n",
    "        super().__init__()\n",
    "        self.n_head = n_head\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attention_hidden_size = self.hidden_size//self.n_head\n",
    "        self.div = self.attention_hidden_size**-0.5\n",
    "        self.mask = torch.triu(torch.full((q_seq_len, k_v_seq_len), float('-inf')), 1).to(device)\n",
    "        self.q_linear = nn.ModuleList(\n",
    "                            [nn.Linear(hidden_size, self.attention_hidden_size)\n",
    "                             for i in range(self.n_head)])\n",
    "\n",
    "        self.k_linear = nn.ModuleList(\n",
    "                            [nn.Linear(hidden_size, self.attention_hidden_size)\n",
    "                             for i in range(self.n_head)])\n",
    "        self.v_linear = nn.Linear(hidden_size, q_seq_len)\n",
    "\n",
    "        self.attention_output_linear = nn.Linear(q_seq_len, self.hidden_size)\n",
    "        self.q_residual = nn.Linear(hidden_size, self.hidden_size)\n",
    "\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "\n",
    "        q_list = [q_linear(q) for q_linear in self.q_linear]\n",
    "        k_list = [torch.transpose(k_linear(k),1,2) for k_linear in self.k_linear]\n",
    "        v = self.v_linear(v)\n",
    "        attention_QK = [torch.matmul(q, k) for q,k in zip(q_list,k_list)]\n",
    "\n",
    "        print(attention_QK[0].size(), self.mask.size())\n",
    "        attention_QK = [F.softmax(torch.mul(attention, self.div) + self.mask, dim =-1) \n",
    "                        for i, attention in enumerate(attention_QK)]\n",
    "        attention_QKV = [torch.matmul(attention, v) for attention in attention_QK]\n",
    "        attention_QKV = torch.stack(attention_QKV, dim=-1)\n",
    "        attention_QKV = torch.mean(attention_QKV, dim=-1) \n",
    "\n",
    "        q_residual = self.q_residual(q)\n",
    "        attention_output = self.attention_output_linear(attention_QKV)\n",
    "        q_sigmoid = self.sigmoid(q_residual)\n",
    "\n",
    "        output = attention_output * q_sigmoid\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossAttention(\n",
       "  (q_linear): ModuleList(\n",
       "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "  )\n",
       "  (k_linear): ModuleList(\n",
       "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "  )\n",
       "  (v_linear): Linear(in_features=64, out_features=7, bias=True)\n",
       "  (attention_output_linear): Linear(in_features=7, out_features=64, bias=True)\n",
       "  (q_residual): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross = CrossAttention( hidden_size = 64,n_head = 1 ,q_seq_len=  7 , k_v_seq_len=5 ,device= device)\n",
    "cross.to(device)\n",
    "cross2 = CrossAttention( hidden_size = 64,n_head = 1 ,q_seq_len=  7 , k_v_seq_len=12 ,device= device)\n",
    "cross2.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch, seq, hidden_size = 32, 7 , 64\n",
    "q = torch.rand(batch, seq, hidden_size, device=device)\n",
    "batch, seq, hidden_size = 32, 5 , 64\n",
    "k,v = torch.rand(batch, seq, hidden_size, device=device), torch.rand(batch, seq, hidden_size, device=device)\n",
    "\n",
    "batch, seq, hidden_size = 32, 12 , 64\n",
    "k_2,v_2 = torch.rand(batch, seq, hidden_size, device=device), torch.rand(batch, seq, hidden_size, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 7, 5]) torch.Size([7, 5])\n"
     ]
    }
   ],
   "source": [
    "out = cross(q,k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 7, 12]) torch.Size([7, 12])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 7, 64])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross2(out, k_2,v_2).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossFusion(nn.Module):\n",
    "    def __init__(self, n_head, num_model, seq_list, hidden_size ,device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cross_list = nn.ModuleList(\n",
    "                            [CrossAttention(n_head = n_head, q_seq_len= seq_list[0] , k_v_seq_len=seq_list[i+1]  ,hidden_size = hidden_size, device= device     )\n",
    "                             for i in range(num_model-1)\n",
    "                             ])\n",
    "        \n",
    "    def forward(self, model_output_list):\n",
    "        q = model_output_list[0]\n",
    "\n",
    "        for i, cross in enumerate(self.cross_list):\n",
    "            q = cross(q, k=model_output_list[i+1], v=model_output_list[i+1])\n",
    "        return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossFusion(\n",
       "  (cross_list): ModuleList(\n",
       "    (0-1): 2 x CrossAttention(\n",
       "      (q_linear): ModuleList(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (k_linear): ModuleList(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (v_linear): Linear(in_features=64, out_features=7, bias=True)\n",
       "      (attention_output_linear): Linear(in_features=7, out_features=64, bias=True)\n",
       "      (q_residual): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = CrossFusion(n_head = 1 , num_model=3,seq_list= [7,5,12], hidden_size=64,device=device )\n",
    "x.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 7, 5]) torch.Size([7, 5])\n",
      "torch.Size([32, 7, 12]) torch.Size([7, 12])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0110, -0.1903,  0.0261,  ...,  0.0489,  0.3566, -0.0579],\n",
       "         [-0.0236, -0.2048,  0.0566,  ...,  0.0444,  0.3262, -0.0403],\n",
       "         [-0.0311, -0.1687,  0.0480,  ...,  0.0373,  0.2983, -0.0593],\n",
       "         ...,\n",
       "         [-0.0106, -0.1697,  0.0370,  ...,  0.0496,  0.2958, -0.0314],\n",
       "         [-0.0200, -0.1619,  0.0303,  ...,  0.0443,  0.2933, -0.0435],\n",
       "         [-0.0246, -0.1644,  0.0386,  ...,  0.0383,  0.2915, -0.0399]],\n",
       "\n",
       "        [[-0.0973, -0.1757,  0.0359,  ..., -0.0096,  0.2621, -0.0943],\n",
       "         [-0.0170, -0.1675, -0.0283,  ...,  0.0605,  0.2745, -0.0756],\n",
       "         [-0.0457, -0.1814, -0.0095,  ...,  0.0439,  0.2913, -0.0891],\n",
       "         ...,\n",
       "         [-0.0406, -0.1841,  0.0193,  ...,  0.0354,  0.3138, -0.0440],\n",
       "         [-0.0414, -0.1947,  0.0239,  ...,  0.0323,  0.3180, -0.0366],\n",
       "         [-0.0407, -0.1916,  0.0181,  ...,  0.0340,  0.3210, -0.0410]],\n",
       "\n",
       "        [[-0.0152, -0.1895,  0.0283,  ...,  0.0387,  0.2900, -0.0363],\n",
       "         [-0.0616, -0.1903,  0.0700,  ...,  0.0103,  0.2813, -0.0559],\n",
       "         [-0.0827, -0.1692,  0.0503,  ...,  0.0015,  0.2683, -0.0849],\n",
       "         ...,\n",
       "         [-0.0443, -0.1426,  0.0435,  ...,  0.0193,  0.2683, -0.0702],\n",
       "         [-0.0482, -0.1467,  0.0229,  ...,  0.0253,  0.2724, -0.0780],\n",
       "         [-0.0504, -0.1646,  0.0146,  ...,  0.0306,  0.2803, -0.0791]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.0682, -0.1576, -0.0517,  ...,  0.0421,  0.2935, -0.0974],\n",
       "         [-0.0482, -0.1851, -0.0282,  ...,  0.0539,  0.2897, -0.0794],\n",
       "         [-0.0260, -0.2187, -0.0430,  ...,  0.0707,  0.3061, -0.0635],\n",
       "         ...,\n",
       "         [-0.0207, -0.2068, -0.0243,  ...,  0.0733,  0.2961, -0.0411],\n",
       "         [-0.0383, -0.1956, -0.0008,  ...,  0.0542,  0.2846, -0.0373],\n",
       "         [-0.0487, -0.1906,  0.0103,  ...,  0.0384,  0.2885, -0.0464]],\n",
       "\n",
       "        [[-0.0388, -0.2010,  0.0445,  ...,  0.0362,  0.3128,  0.0128],\n",
       "         [-0.0383, -0.1942,  0.1008,  ...,  0.0299,  0.2762,  0.0262],\n",
       "         [-0.0379, -0.1734,  0.1171,  ...,  0.0168,  0.2845,  0.0220],\n",
       "         ...,\n",
       "         [-0.0324, -0.1891,  0.1109,  ...,  0.0187,  0.3046,  0.0273],\n",
       "         [-0.0299, -0.1788,  0.0906,  ...,  0.0225,  0.2974,  0.0121],\n",
       "         [-0.0280, -0.1827,  0.0740,  ...,  0.0269,  0.3042, -0.0091]],\n",
       "\n",
       "        [[-0.0855, -0.2095,  0.0589,  ...,  0.0159,  0.3069, -0.0186],\n",
       "         [-0.0327, -0.2034,  0.0104,  ...,  0.0557,  0.2991,  0.0081],\n",
       "         [-0.0214, -0.1844,  0.0033,  ...,  0.0605,  0.2983, -0.0051],\n",
       "         ...,\n",
       "         [-0.0380, -0.1897,  0.0016,  ...,  0.0474,  0.2939, -0.0386],\n",
       "         [-0.0389, -0.1717,  0.0094,  ...,  0.0368,  0.2936, -0.0399],\n",
       "         [-0.0312, -0.1772,  0.0065,  ...,  0.0415,  0.2988, -0.0352]]],\n",
       "       device='cuda:0', grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x([q,k,k_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
